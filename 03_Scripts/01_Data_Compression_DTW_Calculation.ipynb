{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compression and Dynamic Time Warping Distance Notebook\n",
    "Christopher Marais\n",
    "\n",
    "gmarais@ufl.edu\n",
    "\n",
    "2023/12/14\n",
    "\n",
    "<br>\n",
    "\n",
    "This notebook aims to take the taht was extracted and decompress the timestamps of each event. \n",
    "\n",
    "Then we compress it in a different way before calcualting the Dynamic Time Warping (DTW) distance between event timeseries.\n",
    "\n",
    "The input is the csv file of the labels data and the event timestamps list pickle file.\n",
    "\n",
    "The output is a pairwise DTW distance matrix between each event compressed timeseries as a .npy file. \n",
    "\n",
    "##### WARNING!: This notebook can run anywhere beteen 3 - 12 hours. Make sure you need to run it before running it.\n",
    "\n",
    "<br>\n",
    "\n",
    "Input: `/02_Clean_data/00_recording_event_times_labels.csv`, `00_event_timestamps_arrays_list.pkl` \n",
    "\n",
    "↓\n",
    "\n",
    "Process: `</03_Scripts/01_Data_Compression_DTW_Calculation.ipynb>`\n",
    "\n",
    "↓\n",
    "\n",
    "Output: `/02_Clean_data/02_dtw_distance_matrix.npy`\n",
    "\n",
    "<br>\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd # use pandas for more functionality\n",
    "from dtaidistance import dtw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory as parent directory of current directory\n",
    "cwd = os.getcwd()\n",
    "pwd = os.path.dirname(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to decompress and recompress data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decompression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_stream(row):\n",
    "    # Unpack indices and stream length from the row\n",
    "    indices, stream_length = row['event_timestamps'], row['event_length']\n",
    "\n",
    "    # Initialize a NumPy array of zeros\n",
    "    binary_stream = np.zeros(int(stream_length), dtype=int)\n",
    "\n",
    "    if len(indices) != 0:\n",
    "\n",
    "        # Ensure indices are integers\n",
    "        indices = [int(i) for i in indices if isinstance(i, (int, float)) and not np.isnan(i)]\n",
    "\n",
    "        # Convert indices to a NumPy array and filter out-of-bound indices\n",
    "        indices = np.array(indices)\n",
    "        valid_indices = indices[(0 <= indices) & (indices < int(stream_length))]\n",
    "\n",
    "        # Set the specified indices to 1\n",
    "        binary_stream[valid_indices] = 1\n",
    "\n",
    "    return binary_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_zeros(binary_vector, scaling_factor=10):\n",
    "    if scaling_factor <= 0:\n",
    "        raise ValueError(\"Scaling factor must be greater than 0\")\n",
    "\n",
    "    scaled_vector = []\n",
    "    zero_count = 0\n",
    "\n",
    "    for bit in binary_vector:\n",
    "        if bit == 1:\n",
    "            if zero_count > 0:\n",
    "                # Scale the number of zeros and add them to the new vector\n",
    "                scaled_count = max(1, int(math.ceil(zero_count / scaling_factor)))\n",
    "                scaled_vector.extend([0] * scaled_count)\n",
    "                zero_count = 0\n",
    "            scaled_vector.append(1)\n",
    "        else:\n",
    "            zero_count += 1\n",
    "\n",
    "    # Handle trailing zeros\n",
    "    if zero_count > 0:\n",
    "        scaled_count = max(1, int(math.ceil(zero_count / scaling_factor)))\n",
    "        scaled_vector.extend([0] * scaled_count)\n",
    "\n",
    "    # Convert the list to a NumPy array with a double type\n",
    "    return np.array(scaled_vector, dtype=np.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv as a dataframe\n",
    "data_df = pd.read_csv(pwd + \"/02_Clean_data/00_recording_event_times_labels.csv\")\n",
    "# import pickle file with list of arrays\n",
    "with open(pwd + \"/02_Clean_data/00_event_timestamps_arrays_list.pkl\", 'rb') as file:\n",
    "    event_timestamps_arrays_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add arays to dataframe\n",
    "data_df['event_timestamps'] = event_timestamps_arrays_list\n",
    "# get the length of each array\n",
    "data_df['event_length'] = data_df['event_timestamps'].apply(lambda x: len(x))\n",
    "# get the binary stream for each row\n",
    "data_df['binary_stream'] = data_df.apply(create_binary_stream, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create compressed binary representation\n",
    "# Scale factor\n",
    "# smaller = more compressed\n",
    "scale_factor = 100\n",
    "# Apply the function to each element of the column\n",
    "data_df['scaled_arrays'] = data_df['binary_stream'].apply(lambda x: scale_zeros(x, scale_factor))\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping (DTW) distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a small subset to calcuylate the distance matrix from\n",
    "scaled_timeseries_lst = data_df['scaled_arrays'].tolist()\n",
    "\n",
    "# calculate the distance matrix\n",
    "# function docs: https://dtaidistance.readthedocs.io/en/latest/modules/dtw.html?highlight=parallel#dtaidistance.dtw.distance_matrix_fast\n",
    "dtw_distance_matrix = dtw.distance_matrix_fast(scaled_timeseries_lst) # use distance_matrix() if C/OpenMP not working on device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distance matrix for further clustering analysis\n",
    "# Save to .npy file\n",
    "np.save(pwd + \"/02_Clean_data/01_dtw_distance_matrix.npy\", dtw_distance_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
