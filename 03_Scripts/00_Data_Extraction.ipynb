{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import multirecording_spikeanalysis as spike\n",
    "import numpy as np\n",
    "# import pandas as pd # use pandas for more functionality\n",
    "import modin.pandas as pd # use modin to speed things up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define working directory relative to repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory as paerent directory of current directory\n",
    "cwd = os.getcwd()\n",
    "pwd = os.path.dirname(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle files given by lab\n",
    "with open(pwd + '/01_Raw_data/phase2_collection.pkl', 'rb') as f:\n",
    "    phase2 = pickle.load(f)\n",
    "\n",
    "# Not usoing phase 3 data\n",
    "# with open(pwd + '/01_Raw_data/phase3_collection.pkl', 'rb') as f:\n",
    "#     phase3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and reminders about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the subject to which each recording belongs to\n",
    "# .subject\n",
    "\n",
    "# shows the type of unit/neuron (we only use good units in unit_timestamps)\n",
    "# .labels_dict\n",
    "\n",
    "# all the timestamps for all units\n",
    "# .timestamps_var\n",
    "\n",
    "# all the timestamps for each unit\n",
    "# .unit_timestamps \n",
    "\n",
    "# the behaviour labels for each timestamp with a starting and ending time\n",
    "# .event_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rework Data from .pkl files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral labels and event ranges dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata dataframe\n",
    "metadata_df = pd.DataFrame() # create empty dataframe\n",
    "temp_df_lst = [] # create empty list to store dataframes\n",
    "for i, j in phase2.collection.items(): # loop through each recording\n",
    "    temp_df = pd.DataFrame() # create empty dataframe\n",
    "    start_time_lst = [] # create empty list to store start times\n",
    "    end_time_lst = [] # create empty list to store end times\n",
    "    behavior_lab_lst = [] # create empty list to store behavior labels\n",
    "    for k, v in j.event_dict.items(): # loop through each behavior\n",
    "        start_time_lst += list(v[:,0]) # add start times to list\n",
    "        end_time_lst += list(v[:,1]) # add end times to list\n",
    "        behavior_lab_lst += list([k] * len(v)) # add behavior labels to list\n",
    "    temp_df['behavior_label'] = behavior_lab_lst # add behavior labels to dataframe\n",
    "    temp_df['start_time'] = start_time_lst # add start times to dataframe\n",
    "    temp_df['end_time'] = end_time_lst # add end times to dataframe\n",
    "    temp_df['collection_key'] = i # add recording name to dataframe\n",
    "    temp_df['subject'] = j.subject # add subject to dataframe\n",
    "    temp_df_lst.append(temp_df) # add dataframe to list\n",
    "metadata_df = pd.concat(temp_df_lst) # concatenate all dataframes in list to one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Units dataframe & unit timestamps list\n",
    "Save a dataframe of the units used for each subject at each recording session.\n",
    "Extract the timestamps for each event for each unit from the data.\n",
    "Merge metadata dataframes into a single long dataframe. So that metadata can be easily related to each recording session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata dataframe\n",
    "data_df = pd.DataFrame()\n",
    "temp_df_lst = []\n",
    "for i, j in phase2.collection.items():\n",
    "    temp_df = pd.DataFrame()\n",
    "    temp_df['units'] = j.unit_timestamps.keys() # add neurons/units to dataframe\n",
    "    temp_df['timestamps'] = j.unit_timestamps.values() # add neurons/units to dataframe\n",
    "    temp_df['collection_key'] = i # add recording name to dataframe\n",
    "    temp_df['subject'] = j.subject # add subject to dataframe\n",
    "    temp_df_lst.append(temp_df) # add dataframe to list\n",
    "data_df = pd.concat(temp_df_lst) # concatenate all dataframes in list to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "merged_data_df = pd.merge(metadata_df, data_df, on=['collection_key', 'subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control home much before and after event to include with event.\n",
    "BEFORE_EVENT_BUFFER = 1\n",
    "AFTER_EVENT_BUFFER = 0\n",
    "BEFORE_EVENT_BUFFER = BEFORE_EVENT_BUFFER*20000\n",
    "AFTER_EVENT_BUFFER = AFTER_EVENT_BUFFER*20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: <function Series.tolist> is not currently supported by PandasOnDask, defaulting to pandas implementation.\n"
     ]
    }
   ],
   "source": [
    "# get list of timestamps for each unit\n",
    "timestamps_lst = merged_data_df['timestamps'].tolist()\n",
    "# Find the length of the longest array\n",
    "max_length = max(arr.size for arr in timestamps_lst)\n",
    "# make a nan array with the same max shape as the array of timestamps\n",
    "timestamps_array = np.full((40866, max_length), np.nan)\n",
    "# Fill in the nan_filled_array with values from timestamps\n",
    "for i, arr in enumerate(timestamps_lst):\n",
    "    timestamps_array[i, :arr.size] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start and end times for each behavior into arrays\n",
    "min_thresholds = np.array(merged_data_df['start_time']) - BEFORE_EVENT_BUFFER # subtract 20 000 for one extra second before event\n",
    "max_thresholds = np.array(merged_data_df['end_time']) + AFTER_EVENT_BUFFER\n",
    "# Reshape the threshold arrays to column vectors for broadcasting\n",
    "min_thresholds = min_thresholds[:, np.newaxis]\n",
    "max_thresholds = max_thresholds[:, np.newaxis]\n",
    "# Apply thresholds using broadcasting\n",
    "lower_mask = timestamps_array < min_thresholds\n",
    "upper_mask = timestamps_array > max_thresholds\n",
    "# Replace values that are either too low or too high with NaN\n",
    "timestamps_array[lower_mask | upper_mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd # use pandas if modin doesn't work (should be done automatically)\n",
    "# remove nan values from arrays in list\n",
    "event_ts_lst = list(timestamps_array) # convert array to list\n",
    "event_ts_lst = [arr[~np.isnan(arr)] for arr in event_ts_lst] # remove nan values from arrays in list\n",
    "merged_data_df['event_timestamps'] = event_ts_lst # add event timestamps to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the order labels from the behaviour labels to a new column\n",
    "# Get a dataframe that only includes the behaviour labels\n",
    "order_df = merged_data_df[merged_data_df['behavior_label'].isin([\n",
    "    'exposure 1',\n",
    "    'exposure 2', \n",
    "    'exposure 3'])].reset_index(drop=True)\n",
    "order_df = order_df.rename(columns={'behavior_label': 'order'}) # change order label column name\n",
    "# Get a dataframe that only includes the order labels\n",
    "behaviour_df = merged_data_df[merged_data_df['behavior_label'].isin([\n",
    "    'acquisition', \n",
    "    'recall', \n",
    "    'cagemate', \n",
    "    'novel'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes on all columns except the order and behaviour columns\n",
    "merged_labels_data_df = pd.merge(order_df, behaviour_df, on=[\n",
    "    'start_time',\n",
    "    'end_time',\n",
    "    'collection_key',\n",
    "    'subject',\n",
    "    'units'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show table\n",
    "merged_labels_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe as csv file\n",
    "merged_labels_data_df.to_csv(pwd + \"/02_Clean_data/recording_event_times_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
